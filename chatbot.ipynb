{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb573009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\python\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\python\\lib\\site-packages (from pandas) (2022.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\python\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in d:\\python\\lib\\site-packages (from pandas) (1.23.4)\n",
      "Requirement already satisfied: six>=1.5 in d:\\python\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f4f50d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk \n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.stem import wordnet # to perform lemmitization\n",
    "from sklearn.feature_extraction.text import CountVectorizer # to perform bow\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to perform tfidf\n",
    "from nltk import pos_tag # for parts of speech\n",
    "from sklearn.metrics import pairwise_distances # to perfrom cosine similarity\n",
    "from nltk import word_tokenize # to create tokens\n",
    "from nltk.corpus import stopwords # for stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "156246a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wangx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wangx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wangx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\wangx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wangx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\wangx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\wangx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# useful downloads\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfff3af",
   "metadata": {},
   "source": [
    "# Small Talk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d82935eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataframe\n",
    "#some basic responses on small talks\n",
    "df = pd.read_excel('dialog_talk_agent.xlsx')\n",
    "df.ffill(axis = 0,inplace=True) # fills the null value with the previous value.\n",
    "\n",
    "# function that performs text normalization steps\n",
    "# lower case + remove special characters + tokenization + lemmatization\n",
    "def text_normalization(text):\n",
    "    text=str(text).lower() # text to lower case\n",
    "    spl_char_text=re.sub(r'[^ a-z]','',text) # removing special characters\n",
    "    tokens=nltk.word_tokenize(spl_char_text) # word tokenizing\n",
    "    lema=wordnet.WordNetLemmatizer() # intializing lemmatization\n",
    "    tags_list=pos_tag(tokens,tagset=None) # parts of speech\n",
    "    lema_words=[]   # empty list \n",
    "    for token,pos_token in tags_list:\n",
    "        if pos_token.startswith('V'):  # Verb\n",
    "            pos_val='v'\n",
    "        elif pos_token.startswith('J'): # Adjective\n",
    "            pos_val='a'\n",
    "        elif pos_token.startswith('R'): # Adverb\n",
    "            pos_val='r'\n",
    "        else:\n",
    "            pos_val='n' # Noun\n",
    "        lema_token=lema.lemmatize(token,pos_val) # performing lemmatization\n",
    "        lema_words.append(lema_token) # appending the lemmatized token into a list\n",
    "    \n",
    "    return \" \".join(lema_words) # returns the lemmatized tokens as a sentence \n",
    "df['lemmatized_text']=df['Context'].apply(text_normalization) # applying the fuction to the dataset to get clean text\n",
    "\n",
    "\n",
    "# defining a function that returns response to query using tf-idf\n",
    "tfidf=TfidfVectorizer() # intializing tf-id \n",
    "x_tfidf=tfidf.fit_transform(df['lemmatized_text']).toarray() # transforming the data into array\n",
    "\n",
    "df_tfidf=pd.DataFrame(x_tfidf,columns=tfidf.get_feature_names_out()) \n",
    "\n",
    "def chat_tfidf(text):\n",
    "    lemma=text_normalization(text) # calling the function to perform text normalization\n",
    "    tf=tfidf.transform([lemma]).toarray() # applying tf-idf\n",
    "    cos=1-pairwise_distances(df_tfidf,tf,metric='cosine') # applying cosine similarity\n",
    "    index_value=cos.argmax() # getting index number of highest value \n",
    "    return df['Text Response'].loc[index_value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9e331",
   "metadata": {},
   "source": [
    "# Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a499b6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load question and answer success. The length :86821\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def read_corpus():\n",
    "    \"\"\"\n",
    "    read given corpus，write the question list and answer list into qlist, alist \n",
    "    qlist = [\"Q1\"， “Q2”， “Q3” ....]\n",
    "    alist = [\"A1\", \"A2\", \"A3\" ....]\n",
    "    answer matches questions\n",
    "    \"\"\"\n",
    "    qlist = []\n",
    "    alist = []\n",
    "   \n",
    "    with open(\"squad2.0/train-v2.0.json\") as f:\n",
    "        all_data = json.load(f)['data']\n",
    "        for data in all_data:\n",
    "            paragraphs = data['paragraphs']\n",
    "            for paragraph in paragraphs:\n",
    "                for qa in paragraph['qas']:\n",
    "                    # print(qa['id'])\n",
    "                    if qa['answers']:\n",
    "                        qlist.append(qa['question'])\n",
    "                        alist.append(qa['answers'][0]['text'])\n",
    "    assert len(qlist) == len(alist)  # same length\n",
    "    print(\"Load question and answer success. The length :{}\".format(len(qlist)))\n",
    "    return qlist, alist\n",
    "\n",
    "original_qlist, alist = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9c5e9d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}#word frequency\n",
    "for question in original_qlist:\n",
    "    question = question.replace('?', ' ?')\n",
    "    line = question.strip().split()\n",
    "    for word in line:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "sort_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35df3b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed！\n"
     ]
    }
   ],
   "source": [
    "new_qlist = []\n",
    "new_alist = []\n",
    "stopwords =[]\n",
    "porter_stemmer = PorterStemmer()\n",
    "##load stop words\n",
    "with open(\"NLTK's20of20stopwords.txt\") as f1:\n",
    "    lines = f1.readlines()\n",
    "    for line in lines:\n",
    "        stopwords.append(line.strip())\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "for question in original_qlist:\n",
    "    tmp = ''\n",
    "    for sign in ['.', '?', '/', '#', '$', '@', '^', '*', '!', '(', ')']:\n",
    "        question = question.replace(sign, '')\n",
    "    # question = question.replace('?', ' ?')\n",
    "    line = question.strip().split()\n",
    "    for word in line:\n",
    "        try:\n",
    "            if word_freq[word] <= 20:#frequency larger than 20\n",
    "                continue\n",
    "            word = word.lower()\n",
    "        except:\n",
    "            pass\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        for num in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:#contains numbers\n",
    "            if str(num) in word:\n",
    "                word = '1'\n",
    "        word = porter_stemmer.stem(word)# stemming \n",
    "        tmp = tmp + word + \" \"\n",
    "    new_qlist.append(tmp[:-1])\n",
    "    \n",
    "qlist = new_qlist# upadated list\n",
    "print(\"Preprocessing completed！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17d1acd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True, norm='l2') # define one vectorizer of tf-idf\n",
    "\n",
    "tf_idf_model = vectorizer.fit(qlist)\n",
    "X = tf_idf_model.transform(qlist)\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6d7c8ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topresults_invidx(input_q):\n",
    "\n",
    "    #Use the inverted table to filter out the index of candidate issues \n",
    "    quest = input_q.replace('?', ' ?')\n",
    "    line = quest.strip().split()\n",
    "    doc_lst = range(len(qlist))\n",
    "\n",
    "\t#Get the candidates containing the first k characters entered by the user\n",
    "    # for j in range(k):\n",
    "    #     #try:\n",
    "    #     word = line[j]\n",
    "    #     doc = inverted_idx[word]\n",
    "    #     doc_lst = list(set(doc_lst) & set(doc))\n",
    "       \n",
    "\n",
    "    # preprocessing\n",
    "    tmp = ''\n",
    "    for sign in ['.', '?', '/', '#', '$', '@', '^', '*', '!', '(', ')']:\n",
    "        question = input_q.replace(sign, '')\n",
    "    # question = question.replace('?', ' ?')\n",
    "    line = question.strip().split()\n",
    "    for word in line:\n",
    "        try:\n",
    "            if word_freq[word] <= 20:\n",
    "                continue\n",
    "            word = word.lower()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        for num in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]:\n",
    "            if str(num) in word:\n",
    "                word = '1'\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        word = porter_stemmer.stem(word)\n",
    "        tmp = tmp + word + \" \"\n",
    "    input_str = [tmp[:-1]]\n",
    "    \n",
    "\t#representing user input\n",
    "    input_str = tf_idf_model.transform(input_str).toarray()[0]\n",
    "\n",
    "    simlarity = {}\n",
    "    for index in doc_lst:\n",
    "        cos_sim = np.dot(input_str, X[index]) / (np.linalg.norm(input_str) * np.linalg.norm(X[index]) + 1)\n",
    "        simlarity[index] = cos_sim\n",
    "\n",
    "    top_idxs = []  # top_idxs store highest similarity\n",
    "\n",
    "    simlarity = list(sorted(simlarity.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    for _ in range(1):\n",
    "        \n",
    "        index, cos = simlarity[_]\n",
    "        top_idxs.append(index)\n",
    "\n",
    "        \n",
    "\n",
    "    return [alist[indx] for indx in top_idxs]  # highest answers\n",
    "\n",
    "# TODO: test and print results\n",
    "# print (topresults_invidx(\"which president won all of NYC in 1924?\"))\n",
    "# print(topresults_invidx(\"what is Harper Lee's  hometown?\"))\n",
    "# print(topresults_invidx(\"How many people lived in Kathmandu in 2011?\"))\n",
    "# print(topresults_invidx(\"Which area did Beyonce compete for when she was young?\"))\n",
    "# print(topresults_invidx(\"Which area did Beyonce compete for when she was young\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30fda91",
   "metadata": {},
   "source": [
    "# Intent Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8336d56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1591"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qlist = original_qlist[0:1591]\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "small_talk_context = list(df['Context'])[0:1591]\n",
    "intent_data = small_talk_context\n",
    "intent_labels = []\n",
    "\n",
    "for _ in range(1591):\n",
    "    intent_labels.append('small talk') \n",
    "len(intent_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b5e95f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3182\n",
      "3182\n"
     ]
    }
   ],
   "source": [
    "for q in qlist:\n",
    "    intent_data.append(q)\n",
    "for _ in range(len(qlist)):\n",
    "    intent_labels.append('question answering') \n",
    "\n",
    "#print(len(intent_data),len(intent_labels))\n",
    "print(len(intent_data))\n",
    "print(len(intent_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "13354f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_intent_train, X_intent_test, Y_intent_train, Y_intent_test = train_test_split(intent_data, intent_labels, stratify=intent_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "665166e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 139)\t1\n",
      "  (0, 1764)\t1\n",
      "  (0, 146)\t1\n",
      "  (0, 1798)\t1\n",
      "  (0, 291)\t1\n",
      "  (0, 180)\t1\n",
      "  (0, 1809)\t1\n",
      "  (0, 1234)\t1\n",
      "  (0, 346)\t1\n",
      "  (0, 1882)\t1\n",
      "  (0, 353)\t1\n",
      "  (0, 1909)\t1\n",
      "  (1, 882)\t1\n",
      "  (1, 1770)\t1\n",
      "  (2, 1882)\t1\n",
      "  (2, 1909)\t1\n",
      "  (2, 130)\t1\n",
      "  (2, 292)\t1\n",
      "  (2, 1616)\t1\n",
      "  (2, 1722)\t1\n",
      "  (2, 1090)\t1\n",
      "  (2, 945)\t1\n",
      "  (3, 924)\t1\n",
      "  (4, 1082)\t1\n",
      "  (4, 1158)\t1\n",
      "  :\t:\n",
      "  (2540, 139)\t1\n",
      "  (2540, 557)\t1\n",
      "  (2540, 266)\t1\n",
      "  (2540, 402)\t1\n",
      "  (2540, 889)\t1\n",
      "  (2540, 1913)\t1\n",
      "  (2540, 1291)\t1\n",
      "  (2540, 516)\t1\n",
      "  (2540, 1508)\t1\n",
      "  (2540, 500)\t1\n",
      "  (2541, 1969)\t1\n",
      "  (2541, 1863)\t1\n",
      "  (2541, 1464)\t1\n",
      "  (2541, 1625)\t1\n",
      "  (2542, 1969)\t1\n",
      "  (2542, 866)\t1\n",
      "  (2542, 210)\t1\n",
      "  (2542, 921)\t1\n",
      "  (2543, 949)\t1\n",
      "  (2544, 921)\t1\n",
      "  (2544, 273)\t1\n",
      "  (2544, 1972)\t1\n",
      "  (2544, 850)\t1\n",
      "  (2544, 511)\t1\n",
      "  (2544, 811)\t1\n",
      "  (0, 1909)\t0.12483228708897758\n",
      "  (0, 1882)\t0.15958046626974076\n",
      "  (0, 1809)\t0.3762460056189036\n",
      "  (0, 1798)\t0.3401891003449353\n",
      "  (0, 1764)\t0.2848037864251699\n",
      "  (0, 1234)\t0.32268611122021496\n",
      "  (0, 353)\t0.30413219507096706\n",
      "  (0, 346)\t0.3068004270263039\n",
      "  (0, 291)\t0.17486954177173605\n",
      "  (0, 180)\t0.21498193770429008\n",
      "  (0, 146)\t0.4239106417152545\n",
      "  (0, 139)\t0.2707435217523356\n",
      "  (1, 1770)\t0.6687556225083955\n",
      "  (1, 882)\t0.7434822912238115\n",
      "  (2, 1909)\t0.1445630242045463\n",
      "  (2, 1882)\t0.18480342983287615\n",
      "  (2, 1722)\t0.4909130945839904\n",
      "  (2, 1616)\t0.4491571082809728\n",
      "  (2, 1090)\t0.46648740841609887\n",
      "  (2, 945)\t0.1846166354061037\n",
      "  (2, 292)\t0.22085184582140435\n",
      "  (2, 130)\t0.4491571082809728\n",
      "  (3, 924)\t1.0\n",
      "  (4, 1969)\t0.24398027245780388\n",
      "  (4, 1758)\t0.6360735593606245\n",
      "  :\t:\n",
      "  (2540, 1913)\t0.3404835887804338\n",
      "  (2540, 1508)\t0.37936617378535864\n",
      "  (2540, 1291)\t0.39550390463129975\n",
      "  (2540, 889)\t0.23768362574072122\n",
      "  (2540, 557)\t0.14980798605948878\n",
      "  (2540, 516)\t0.35662131962637483\n",
      "  (2540, 500)\t0.4343864896362246\n",
      "  (2540, 402)\t0.16490150596162892\n",
      "  (2540, 266)\t0.2765662107456461\n",
      "  (2540, 139)\t0.29196094115897897\n",
      "  (2541, 1969)\t0.26596424265635843\n",
      "  (2541, 1863)\t0.5103617659360554\n",
      "  (2541, 1625)\t0.6724480413546173\n",
      "  (2541, 1464)\t0.4654111313421709\n",
      "  (2542, 1969)\t0.3142335359991714\n",
      "  (2542, 921)\t0.4540824387699831\n",
      "  (2542, 866)\t0.7147704594247833\n",
      "  (2542, 210)\t0.42914987357314505\n",
      "  (2543, 949)\t1.0\n",
      "  (2544, 1972)\t0.3582426026270634\n",
      "  (2544, 921)\t0.2941627439748967\n",
      "  (2544, 850)\t0.40768252207979977\n",
      "  (2544, 811)\t0.4178056193443982\n",
      "  (2544, 511)\t0.43193629126519867\n",
      "  (2544, 273)\t0.5077351510808663\n",
      "[[317   2]\n",
      " [  5 313]]\n",
      "0.989010989010989\n",
      "0.9889415481832543\n",
      "['small talk']\n",
      "small talk\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "p_stemmer = PorterStemmer()\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (p_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "intent_count_vect = CountVectorizer(lowercase=True, stop_words=stopwords.words('english'), analyzer=stemmed_words)\n",
    "X_intent_train_counts = intent_count_vect.fit_transform(X_intent_train)\n",
    "\n",
    "print(X_intent_train_counts)\n",
    "## Weighting\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "intent_tfidf_transformer = TfidfTransformer(use_idf=True, sublinear_tf=True).fit(X_intent_train_counts)\n",
    "X_intent_train_tf = intent_tfidf_transformer.transform(X_intent_train_counts)\n",
    "\n",
    "print(X_intent_train_tf)\n",
    "## Training a classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "intent_clf = LogisticRegression(random_state=0).fit(X_intent_train_tf, Y_intent_train)\n",
    "## Evaluating a classifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Preprocessing documents and creating term-document matrix\n",
    "X_intent_new_counts = intent_count_vect.transform(X_intent_test)\n",
    "\n",
    "# Weighting\n",
    "X_new_tfidf = intent_tfidf_transformer.transform(X_intent_new_counts)\n",
    "\n",
    "# Predict on the test set\n",
    "predicted = intent_clf.predict(X_new_tfidf)\n",
    "\n",
    "# Print metrics\n",
    "print(confusion_matrix(Y_intent_test, predicted))\n",
    "print(accuracy_score(Y_intent_test, predicted))\n",
    "print(f1_score(Y_intent_test, predicted, pos_label='small talk'))\n",
    "\n",
    "# Using the classifier on new data\n",
    "#Preprocessing and creating term-document matrix\n",
    "intent_new_data = [\"who are you?\"]\n",
    "intent_processed_newdata = intent_count_vect.transform(intent_new_data)\n",
    "\n",
    "#Weighting\n",
    "processed_newdata = intent_tfidf_transformer.transform(intent_processed_newdata)\n",
    "\n",
    "# Predict\n",
    "print(intent_clf.predict(processed_newdata))\n",
    "print(''.join(intent_clf.predict(processed_newdata)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516ecd89",
   "metadata": {},
   "source": [
    "# Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54ceb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stores user information\n",
    "class User:\n",
    "    def __init__(self) :\n",
    "        self.storage = {}\n",
    "\n",
    "    def add_name(self,userName):\n",
    "        self.storage['name'] = userName\n",
    "    \n",
    "    def add_hometown(self,home):\n",
    "        self.storage['hometown'] = home\n",
    "\n",
    "    def add_company(self,company):\n",
    "        self.storage['company'] = company\n",
    "    \n",
    "    def get_name(self):\n",
    "        return self.storage['name']\n",
    "    \n",
    "    def get_hometown(self):\n",
    "        return self.storage['hometown'] \n",
    "    \n",
    "    def get_company(self):\n",
    "        return self.storage['company']\n",
    "\n",
    "        # self.name = userName\n",
    "        # self.company = company\n",
    "        # self.hometown = hometown\n",
    "        # self.storage = {\"username\":self.name,\"company\":self.company,\"hometwon\":self.company}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95a17b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "import re\n",
    "def identification(text):\n",
    "    results = pos_tag(word_tokenize(text))\n",
    "    info = []\n",
    "    for x in results:\n",
    "        if 'NNP' in x:    # Ends with 'NNP'\n",
    "            info.append(x[0])\n",
    "    info_text = \" \".join(info)\n",
    "    return info_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40c6051b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sorry, could you pleas tell me your name?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user = User()\n",
    "name_triggered = [\"my name is\",\"call me\"]\n",
    "hometown_triggered = [\"my hometown is\",\"I'm from\"]\n",
    "org_triggered = [\"my company is\",\"I'm working for\"]\n",
    "name_required = \"What's my name?\"\n",
    "hometown_required = \"Where am I from?\"\n",
    "org_required = \"Which company am I with?\"\n",
    "GREETING_INPUTS=[\"Hello\",\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\",\"hey!\",\"How are you?\",\"How are you doing?\"]\n",
    "\n",
    "def user_profile(text):\n",
    "\n",
    "    for i in range(2):\n",
    "\n",
    "        if name_triggered[i] in text or name_triggered[i] in text.lower():\n",
    "            \n",
    "            name = identification(text)\n",
    "            #print(name)\n",
    "            user.add_name(str(name))\n",
    "            return \"Nice to meet you. I'm your chatbot and you  can call me Teresa!\"\n",
    "        if hometown_triggered[i] in text or hometown_triggered[i] in text.lower():\n",
    "            hometown = identification(text)\n",
    "            user.add_hometown(hometown)\n",
    "            return \"Thank you for telling me.\"\n",
    "        if  org_triggered[i] in text or org_triggered[i] in text.lower():\n",
    "            company = identification(text)\n",
    "            user.add_company(company)\n",
    "            return \"Glad to know about that.\"\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    if text in name_required or text in name_required.lower():\n",
    "        try:\n",
    "            return user.get_name()\n",
    "        except:\n",
    "            return \"Sorry, could you pleas tell me your name?\"\n",
    "    if text in hometown_required or text in hometown_required.lower():\n",
    "        try:\n",
    "            return user.get_hometown()\n",
    "        except:\n",
    "            return \"Sorry, could you pleas tell me your hometown?\"\n",
    "    if (text in org_required or text ==  org_required.lower()) and text not in GREETING_INPUTS:\n",
    "        try:\n",
    "            return user.get_company()\n",
    "        except:\n",
    "            return \"Sorry, could you pleas tell me your company?\"\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "#user_profile(\"what's my name\")\n",
    "# user_profile(\"I'm working for GOOGLE\")\n",
    "# print(user_profile(\"Which company am I with?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35648a7c",
   "metadata": {},
   "source": [
    "# Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c268329",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "label_dir = { # 500//500\n",
    "    \"positive\": \"data/positive\",\n",
    "    \"negative\": \"data/negative\"\n",
    "}\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for label in label_dir.keys():\n",
    "    for file_name in os.listdir(label_dir[label]):\n",
    "        filepath = f\"{label_dir[label]}/{file_name}\"\n",
    "        with open(filepath, mode='r', encoding='utf8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "            data.append(content)\n",
    "            labels.append(label)\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify=labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb30d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(lowercase=True, stop_words=stopwords.words('english'), analyzer=stemmed_words)\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "\n",
    "#print(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0e39a6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True, sublinear_tf=True).fit(X_train_counts)\n",
    "X_train_tf = tfidf_transformer.transform(X_train_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fff72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train_tf, y_train)\n",
    "X_new_counts = count_vect.transform(X_test)\n",
    "\n",
    "# Weighting\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "28159e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So a young man comes to his first ever Karate lesson\n",
      "He steps through the doors of the dojo and sees three groups being taught moves by an instructor\n",
      "\n",
      "He is directed to the first line where one of the Sensei's is teaching them how to block a hit\n",
      "\n",
      "The man quickly learns the move and advances to the second group, proud of his achievement\n",
      "\n",
      "The second line is taught one by one to perform a simple throw, but the man struggles as he has always lacked upper body strength\n",
      "\n",
      "After many tries he finally succeeds but he decides karate is just not for him.\n",
      "\n",
      "The young man turns around and walks towards the door, however on his way out the Sensei calls out his name and says:\n",
      "\n",
      "\"Hey, didn't you forget the punch line?\"\n"
     ]
    }
   ],
   "source": [
    "# offering a joke\n",
    "import json\n",
    "import random\n",
    "\n",
    "def shuffle_jokes():\n",
    "    \"\"\"\n",
    "\n",
    "    Retrieve the given json file containing the joke and write the joke to the joke_List. \n",
    "    joke_list = [\"joke1\"， “joke2”， “joke3” ....]\n",
    "    \n",
    "    \"\"\"\n",
    "    joke_list = []\n",
    "    \n",
    "   \n",
    "    with open(\"reddit_jokes.json\") as f:\n",
    "        all_jokes = json.load(f)\n",
    "        for joke in all_jokes:\n",
    "            tell_joke = joke['title'] + \"\\n\" +joke[\"body\"]\n",
    "            joke_list.append(tell_joke)\n",
    "\n",
    "    randnum = random.randint(0,194999)\n",
    "    return joke_list[randnum]\n",
    "\n",
    "#print(shuffle_jokes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6cf8c797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_emotions(new_data):\n",
    "    \n",
    "    processed_newdata = count_vect.transform(new_data)\n",
    "\n",
    "    # Weighting\n",
    "    processed_newdata = tfidf_transformer.transform(processed_newdata)\n",
    "    emotion_state = ''.join(clf.predict(processed_newdata))\n",
    "    return emotion_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb9c7f",
   "metadata": {},
   "source": [
    "# Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71bc9eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>hye!\n",
      "Chatbot: Just think of me as the ace up your sleeve.\n",
      ">>>hello\n",
      "Chatbot: Howdy.\n",
      ">>>hope you have a good day\n",
      "Chatbot: Lovely, thanks.\n",
      ">>>call me Wang\n",
      "Chatbot: Nice to meet you. I'm your chatbot and you  can call me Teresa!\n",
      ">>>I'm working for GOOGLE\n",
      "Chatbot: Glad to know about that.\n",
      ">>>My hometown is BeiJing\n",
      "Chatbot: Thank you for telling me.\n",
      ">>>what's my name\n",
      "Chatbot: Wang\n",
      ">>>which company am I with\n",
      "Chatbot: Because medical research and development of drugs to treat such diseases is financially disadvantageous\n",
      ">>>which company am I with\n",
      "Chatbot: Because medical research and development of drugs to treat such diseases is financially disadvantageous\n",
      ">>>Which company am I with\n",
      "Chatbot: GOOGLE\n",
      ">>>where am I from\n",
      "Chatbot: I wish I knew where.\n",
      ">>>Where am I from\n",
      "Chatbot: BeiJing\n",
      ">>>python is a good language\n",
      "Chatbot: Great! Glad to hear it.\n",
      ">>>today is awful\n",
      "Chatbot: I'm sorry. Please let me know if I can help in some way.\n",
      "Chatbot:I think you are unhappy.Do you need me to tell you a joke?\n",
      ">>>no\n",
      "Chatbot:Okay,hope you have a good day!\n",
      ">>>today is awful\n",
      "Chatbot: I'm sorry. Please let me know if I can help in some way.\n",
      "Chatbot:I think you are unhappy.Do you need me to tell you a joke?\n",
      ">>>yes\n",
      "My friend sure changed when she became a vegetarian... (fixed)\n",
      "I mean, she never actually told me she was a vegetarian. But they say you are what you eat. And now she's a vegetable.\n",
      ">>>hich area did Beyonce compete for when she was young?\n",
      "Chatbot: singing and dancing\n",
      ">>>How many people lived in Kathmandu in 2011?\n",
      "Chatbot: 975,453\n",
      ">>>bye\n",
      "Chatbot: Bye!\n"
     ]
    }
   ],
   "source": [
    "GREETING_INPUTS=[\"Hello\",\"hello\",\"hi\",\"greetings\",\"sup\",\"what's up\",\"hey\",\"hey!\",\"How are you?\",\"How are you doing?\"]\n",
    "\n",
    "while True:\n",
    "    userInput = input(\">>>\")\n",
    "    botOutput = user_profile(userInput)\n",
    "    if chat_tfidf(userInput) == 'Bye.':\n",
    "        print('Chatbot: Bye!')\n",
    "        break\n",
    "    #first phase:check the intent\n",
    "    elif userInput in GREETING_INPUTS or userInput.lower() in GREETING_INPUTS or botOutput != None:\n",
    "        intent = 'small talk'\n",
    "    else:\n",
    "        intent_input = intent_count_vect.transform([userInput])\n",
    "        intent_input_proceed= intent_tfidf_transformer.transform(intent_input)\n",
    "        intent = ''.join(intent_clf.predict(intent_input_proceed))\n",
    "    #print(intent)\n",
    "\n",
    "    #small talk or question answring\n",
    "    if intent == 'small talk':\n",
    "        \n",
    "        if botOutput != None:\n",
    "            print(\"Chatbot: \" +  user_profile(userInput))\n",
    "        else:\n",
    "            print(\"Chatbot: \"+chat_tfidf(userInput))  #just small talks\n",
    "            \n",
    "            emo = predicted_emotions([userInput])\n",
    "            if  emo == 'negative':\n",
    "                print(\"Chatbot:I think you are unhappy.Do you need me to tell you a joke?\")\n",
    "                while True:\n",
    "                    instant_input = [input(\">>>\")]\n",
    "                    if 'yes' in instant_input:\n",
    "                        print(shuffle_jokes())\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Chatbot:Okay,hope you have a good day!\")\n",
    "                        break\n",
    "    else:\n",
    "        try:\n",
    "            answer = topresults_invidx(userInput)\n",
    "            if len(answer) == 0:\n",
    "                print(\"Chatbot: I can't do anything for now but my developer will implement more NLP in the future\")\n",
    "            else:\n",
    "                print(\"Chatbot: \" + ''.join(answer))\n",
    "        except:\n",
    "            print(\"Sorry, I cannot totally understand. Could you please change your expression?\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62cf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "865d8b2eb28e274047ba64063dfb6a2aabf0dfec4905d304d7a76618dae6fdd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
